# EDA_PMEMO_Dataset
 
The PMEmo dataset contains chorus sections of 794 music clips annotated by 457 subjects on the two-dimensional VA axis in the range [0, 1]. 

The chorus sections were manually selected by participants majoring in music studies and the songs were carefully chosen from Billboard Hot 100, iTunes Top 100 and UK Top 40 charts covering a wide range of popular music. 

Out of 457 annotators, 413 had a non-music major background and a mix of English and Chinese speaking annotators were recruited to reduce the impact of cultural background. 

Each song was annotated by at least 10 annotators. The filtering of annotated data is done to avoid the crowd-sourcing bias and the inter-annotator agreement measured using Cronbach’s $\alpha$ is $\alpha_{Valence} = 0.998$ and $\alpha_{Arousal} = 0.998$, indicating that the annotations are of high quality compared with other datasets. 

Figure shows the scatter plot of the average values (per song) of VA annotations of 794 songs, along with the marginal VA
distributions. It can be inferred that most songs are located in the first quadrant, implying that popular music results in high valence and high arousal perception in listeners.

![image](https://user-images.githubusercontent.com/17112412/208496482-11aa9917-8593-4ddd-8088-a88e46bc5d6a.png)

Ref: Zhang K, Zhang H, Li S, Yang C and Sun L, "The PMEmo dataset for music emotion recognition", In: Proceedings of the 8th International Conference on Multimedia Retrieval, ICMR 2018, pp. 135–142, 2018.
